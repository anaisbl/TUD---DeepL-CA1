{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Image Processing & Transfer learning\n",
    "This notebook contains all the models for basic modelling, investigating loss functions, activation functions, skip connection, autoencoding models and transfer learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Add, Concatenate, UpSampling2D, Dropout\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import SparseCategoricalCrossentropy\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Block 1 data\n",
    "x_block1 = np.load(\"p2_data/block1/train/x_train.npy\")\n",
    "y_block1 = np.load(\"p2_data/block1/train/y_train.npy\")\n",
    "x_val_block1 = np.load(\"p2_data/block1/val/x_val.npy\")\n",
    "y_val_block1 = np.load(\"p2_data/block1/val/y_val.npy\")\n",
    "\n",
    "# normalize pixel values to 0-1\n",
    "x_train_block1 = x_block1.astype('float32') / 255\n",
    "x_val_block1 = x_val_block1.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global\n",
    "Variables and functions to be used all throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for base cnn model\n",
    "def create_model(activation_func):\n",
    "    model = Sequential([\n",
    "        Conv2D(16, kernel_size=(3,3), activation=activation_func, input_shape=(32,32,3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(32, kernel_size=(3,3), activation=activation_func),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64, kernel_size=(3,3), activation=activation_func),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=activation_func),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation=activation_func),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation=activation_func),\n",
    "        Dropout(0.5),\n",
    "        Dense(50, activation=\"softmax\") \n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# function to plot accuracy & loss trend during training\n",
    "def training_results(result):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # plot accuracy\n",
    "    axs[0].plot(result.history['accuracy'], label='accuracy')\n",
    "    axs[0].plot(result.history['val_accuracy'], label='val_accuracy')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].set_ylabel('Accuracy')\n",
    "    axs[0].set_ylim([0.0, 1.0])\n",
    "    axs[0].legend(loc='lower right')\n",
    "\n",
    "    # plot loss\n",
    "    axs[1].plot(result.history['loss'], label='loss')\n",
    "    axs[1].plot(result.history['val_loss'], label='val_loss')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].set_ylim([0.0, 5.0])\n",
    "    axs[1].legend(loc='upper right')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Modelling - Block 1\n",
    "Classify images using CNN with 5 hidden layers, investigate use of activation functions, different losses & skip connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create base model\n",
    "activ_func = \"relu\"\n",
    "base_model = create_model(activ_func)\n",
    "\n",
    "# compile & fit model\n",
    "base_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "result = base_model.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# evaluate model\n",
    "test_loss, test_acc = base_model.evaluate(x_val_block1, y_val_block1)\n",
    "print(test_acc)\n",
    "\n",
    "# plot results\n",
    "training_results(result)\n",
    "base_model.save(\"models_p2/base.keras\", save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating activation functions\n",
    "Applying different activation functions to the hidden layers of the base cnn model, and to the output layer of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1 - No activation function hidden\n",
    "activ_func1 = \"None\"\n",
    "model1 = create_model(activ_func)\n",
    "\n",
    "model1.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "result1 = model1.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss1, test_acc1 = model1.evaluate(x_val_block1, y_val_block1, verbose=2)\n",
    "print(test_acc1)\n",
    "training_results(result1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2 - tanh hidden\n",
    "activ_func2 = \"tanh\"\n",
    "model2 = create_model(activ_func)\n",
    "\n",
    "model2.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "result2 = model2.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss2, test_acc2 = model2.evaluate(x_val_block1, y_val_block1, verbose=2)\n",
    "print(test_acc2)\n",
    "training_results(result2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3 - sigmoid hidden\n",
    "activ_func3 = \"sigmoid\"\n",
    "model3 = create_model(activ_func)\n",
    "\n",
    "model3.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "result3 = model3.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss3, test_acc3 = model3.evaluate(x_val_block1, y_val_block1, verbose=2)\n",
    "print(test_acc3)\n",
    "training_results(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 4 - sigmoid activation output\n",
    "model4 = Sequential([\n",
    "        Conv2D(16, kernel_size=(3,3), activation=\"relu\", input_shape=(32,32,3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(32, kernel_size=(3,3), activation=\"relu\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64, kernel_size=(3,3), activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(50, activation=\"sigmoid\") \n",
    "    ])\n",
    "\n",
    "model4.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "result4 = model4.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss4, test_acc4 = model4.evaluate(x_val_block1, y_val_block1, verbose=2)\n",
    "print(test_acc4)\n",
    "training_results(result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 5 - no activation/raw logits output\n",
    "model5 = Sequential([\n",
    "        Conv2D(16, kernel_size=(3,3), activation=\"relu\", input_shape=(32,32,3)),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(32, kernel_size=(3,3), activation=\"relu\"),\n",
    "        MaxPooling2D((2,2)),\n",
    "        Conv2D(64, kernel_size=(3,3), activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(256, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dropout(0.5),\n",
    "        Dense(50) \n",
    "    ])\n",
    "\n",
    "model5.compile(optimizer=\"adam\", loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "result5 = model5.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss5, test_acc5 = model5.evaluate(x_val_block1, y_val_block1, verbose=2)\n",
    "print(test_acc5)\n",
    "training_results(result5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating different loss functions\n",
    "Testing base cnn model with different loss functions during model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode  labels\n",
    "y_block1_ohc = to_categorical(y_block1, num_classes=50)\n",
    "y_val_block1_ohc = to_categorical(y_val_block1, num_classes=50)\n",
    "\n",
    "# Model 6 - Categorical cross entropy\n",
    "model6 = base_model\n",
    "model6.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "result6 = model6.fit(x_train_block1, y_block1_ohc, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss6, test_acc6 = model6.evaluate(x_val_block1, y_val_block1_ohc, verbose=2)\n",
    "print(test_acc6)\n",
    "training_results(result6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating skip connections\n",
    "Adding a residual block in the base cnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function defining layers & residual block that will skip connections\n",
    "def residual_block(x, filters, activation='relu'):\n",
    "    shortcut = x\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), activation=activation, padding='same')(x)\n",
    "    x = Conv2D(filters, kernel_size=(3, 3), activation=None, padding='same')(x)\n",
    "    x = Add()([shortcut, x])\n",
    "    return x\n",
    "\n",
    "# function for model that will contain residual block\n",
    "def model_skip_connect():\n",
    "    inputs = Input(shape=(32, 32, 3))\n",
    "    x = Conv2D(16, kernel_size=(3,3), activation=\"relu\")(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "\n",
    "    x = residual_block(x, filters=16)\n",
    "    \n",
    "    x = Conv2D(32, kernel_size=(3,3), activation=\"relu\")(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = residual_block(x, filters=32)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation=\"relu\")(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(50, activation=\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# create, compile & evaluate model\n",
    "model7 = model_skip_connect()\n",
    "model7.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "result7 = model7.fit(x_train_block1, y_block1, epochs=10, batch_size=47, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "test_loss7, test_acc7 = model7.evaluate(x_val_block1, y_val_block1, verbose=2)\n",
    "print(test_acc7)\n",
    "training_results(result7)\n",
    "\n",
    "#model7.summary()\n",
    "model7.save(\"models_p2/skip_connect.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder modelling - Block 2\n",
    "Construct autoencoder model to attempt to reproduce images from CIFAR100 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build autoencoder\n",
    "def build_autoencoder(input_shape):\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    \n",
    "    # encoder\n",
    "    x = Conv2D(16, kernel_size=(3,3), activation='relu', padding=\"same\")(encoder_input)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(32, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    encoded = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # decoder\n",
    "    x = Conv2D(32, kernel_size=(3,3), activation='relu', padding=\"same\")(encoded)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(16, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    decoded = Conv2D(3, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\n",
    "    \n",
    "    # combine encoder & decoder\n",
    "    autoencoder = Model(encoder_input, decoded)\n",
    "    return autoencoder\n",
    "\n",
    "# build autoencoder model\n",
    "autoencode_model = build_autoencoder(input_shape=(32, 32, 3))\n",
    "autoencode_model1 = build_autoencoder(input_shape=(32,32,3))\n",
    "\n",
    "# compile & fit model\n",
    "autoencode_model1.compile(optimizer=\"adam\", loss='binary_crossentropy')\n",
    "autoencode_model.compile(optimizer=\"adam\", loss='mean_squared_error')\n",
    "\n",
    "result8 = autoencode_model.fit(x_train_block1, x_train_block1, epochs=5, batch_size=47, shuffle=True, validation_split=0.1)\n",
    "result9 = autoencode_model1.fit(x_train_block1, x_train_block1, epochs=5, batch_size=47, shuffle=True, validation_split=0.1)\n",
    "\n",
    "# evaluate model\n",
    "evaluation_loss = autoencode_model.evaluate(x_val_block1, x_val_block1, verbose=2)\n",
    "print(\"Evaluation Loss:\", evaluation_loss)\n",
    "\n",
    "# evaluate model\n",
    "evaluation_loss1 = autoencode_model1.evaluate(x_val_block1, x_val_block1, verbose=2)\n",
    "print(\"Evaluation Loss:\", evaluation_loss1)\n",
    "\n",
    "# save model\n",
    "autoencode_model.save(\"models_p2/autoencode.keras\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct images\n",
    "def vis_reconstruct_img(model, original_img, n=3):\n",
    "    reconstructed_images = model.predict(original_img)\n",
    "    n = 3 \n",
    "    plt.figure(figsize=(9, 4))\n",
    "    for i in range(n):\n",
    "        # og images\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(original_img[i])\n",
    "        plt.title(\"Original\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # reconstructed images\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(reconstructed_images[i])\n",
    "        plt.title(\"Reconstructed\")\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "    plt.show()\n",
    "\n",
    "vis_reconstruct_img(autoencode_model, x_val_block1)\n",
    "vis_reconstruct_img(autoencode_model1, x_val_block1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning - Block 2\n",
    "Using autoencoder model & skip connect model above, investigate use of transfer learning to build a network for Block 2 images - compare results with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train data - Block 2\n",
    "x_train_block2 = np.load(\"p2_data/block2/train/x_train.npy\")\n",
    "y_train_block2 = np.load(\"p2_data/block2/train/y_train.npy\")\n",
    "x_val_block2 = np.load(\"p2_data/block2/val/x_val.npy\")\n",
    "y_val_block2 = np.load(\"p2_data/block2/val/y_val.npy\")\n",
    "\n",
    "# normalize pixel values\n",
    "x_train_block2 = x_train_block2.astype('float32') / 255\n",
    "x_val_block2 = x_val_block2.astype('float32') / 255\n",
    "\n",
    "# load models\n",
    "tf1_model = load_model(\"models_p2/skip_connect.keras\")\n",
    "tf2_model = load_model(\"models_p2/autoencode.keras\")\n",
    "\n",
    "# input layer\n",
    "input_layer = Input(shape=(32, 32, 3))\n",
    "\n",
    "# select convolutional layers of skip connect model\n",
    "tf1_layers = [layer for layer in tf1_model.layers if isinstance(layer, Conv2D)]\n",
    "tf1_output = input_layer\n",
    "for layer in tf1_layers: \n",
    "    tf1_output = layer(tf1_output)\n",
    "tf1_flattened = Flatten()(tf1_output)\n",
    "\n",
    "# select layers in second model\n",
    "for layer in tf2_model.layers: # to freeze\n",
    "    layer.trainable = False\n",
    "tf2_output = tf2_model(input_layer)\n",
    "tf2_flattened = Flatten()(tf2_output)\n",
    "\n",
    "# concat layers from the models\n",
    "combined_output = Concatenate()([tf2_flattened, tf1_flattened])\n",
    "\n",
    "# define transfer learning model\n",
    "x = Dense(256, activation=\"relu\")(combined_output)\n",
    "x = Dropout(0.2)(x)\n",
    "outputs2 = Dense(50, activation=\"softmax\")(x)\n",
    "\n",
    "# create transfer model\n",
    "transfer_model = Model(input_layer, outputs2)\n",
    "\n",
    "# compile & fit transfer model\n",
    "transfer_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "result9 = transfer_model.fit(x_train_block2, y_train_block2, epochs=5, validation_split=0.1,batch_size=35)\n",
    "\n",
    "# evaluate model\n",
    "transfer_loss, transfer_accuracy = transfer_model.evaluate(x_val_block2, y_val_block2)\n",
    "\n",
    "print(\"Test Loss:\", transfer_loss)\n",
    "print(\"Test Accuracy:\", transfer_accuracy)\n",
    "\n",
    "training_results(result9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
