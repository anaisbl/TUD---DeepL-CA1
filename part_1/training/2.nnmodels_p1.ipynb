{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 - Neural Network Models\n",
    "This notebook contains preliminary data exploration, data processing and training for all RNN & CNN models, variants and embeddings for Genre prediction using 1 input feature: song lyrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import file\n",
    "music_df = pd.read_csv(\"p1_data/sample_train.csv\", index_col=False, sep=\",\", quotechar='\"')\n",
    "\n",
    "# view class distribution\n",
    "print(music_df.groupby([\"Genre\"])[\"Genre\"].count())\n",
    "\n",
    "# lyric length calculation\n",
    "music_df[\"Lyric_Length\"] = music_df[\"Lyrics\"].apply(len)\n",
    "text_length_distribution = music_df[\"Lyric_Length\"].describe()\n",
    "\n",
    "# artist length calculation\n",
    "music_df[\"Artist_Length\"] = music_df[\"Artist\"].apply(len)\n",
    "text_length_distribution = music_df[\"Artist_Length\"].describe()\n",
    "\n",
    "# visualize both distributions\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# lyric length distribution\n",
    "axs[0].hist(music_df[\"Lyric_Length\"])\n",
    "axs[0].set_xlabel(\"Text Length\")\n",
    "axs[0].set_ylabel(\"Number of Songs\")\n",
    "axs[0].set_title(\"Distribution of Text Lengths in Lyrics\")\n",
    "\n",
    "# artist length distribution\n",
    "axs[1].hist(music_df[\"Artist_Length\"], color=\"green\")\n",
    "axs[1].set_xlabel(\"Text Length\")\n",
    "axs[1].set_ylabel(\"Number of Songs\")\n",
    "axs[1].set_title(\"Distribution of Text Lengths in Artist\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# function to lowercase, remove punctuation & stopwords\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    return filtered_text\n",
    "\n",
    "# preprocess lyrics\n",
    "music_df[\"Prsd_Lyrics\"] = music_df[\"Lyrics\"].apply(preprocess_text)\n",
    "\n",
    "# extract labels & convert to one-hot encoded vectors\n",
    "labels = music_df[\"Genre\"]\n",
    "label_dict = {label: i for i, label in enumerate(labels.unique())}\n",
    "labels_encoded = labels.map(label_dict)\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels_encoded)\n",
    "\n",
    "# tokenize and pad lyrics\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(music_df['Prsd_Lyrics'])\n",
    "sequences = tokenizer.texts_to_sequences(music_df['Prsd_Lyrics'])\n",
    "vocab_size = len(tokenizer.word_index) + 1 # number of unique words\n",
    "max_text_length = 4000 # chosen based on distribution above, excluding extreme values\n",
    "X = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_text_length)\n",
    "\n",
    "# randomly shuffle data indices of dataframe\n",
    "data_indices = list(range(len(music_df)))\n",
    "random.shuffle(data_indices)\n",
    "\n",
    "# split data into train/test using indices\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(music_df) * split_ratio)\n",
    "train_indices = data_indices[:split_index]\n",
    "test_indices = data_indices[split_index:]\n",
    "\n",
    "X_train = X[train_indices]\n",
    "X_test = X[test_indices]\n",
    "y_train = labels_categorical[train_indices]\n",
    "y_test = labels_categorical[test_indices]\n",
    "\n",
    "# reshape for RNN & CNN architecture compatibility\n",
    "X_train = X_train[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]\n",
    "X_train = X_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Models\n",
    "Exploring Simple RNN, LSTM and dense layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# implement Early Stopping for overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple rnn model\n",
    "rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=64, return_sequences=True),\n",
    "    tf.keras.layers.SimpleRNN(units=16, dropout=0.2),\n",
    "    tf.keras.layers.Dense(units=len(label_dict), activation=\"softmax\") # final output layer\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "rnn_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# train & evaluate the model\n",
    "rnn_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "rnn_loss, rnn_accuracy = rnn_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", rnn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.summary()\n",
    "rnn_model.save(\"models_p1/rnn/rnn.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "lstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(units=64),\n",
    "    tf.keras.layers.Dense(units=len(label_dict), activation=\"softmax\") # final output layer\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "lstm_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# train & evaluate the model\n",
    "lstm_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "lstm_loss, lstm_accuracy = lstm_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", lstm_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM RNN model\n",
    "lstm_rnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.SimpleRNN(units=8, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=16),\n",
    "    tf.keras.layers.Dense(units=len(label_dict), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# compile the model\n",
    "lstm_rnn_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# train & evaluate the model\n",
    "lstm_rnn_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "lstm_rnn_loss, lstm_rnn_accuracy = lstm_rnn_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", lstm_rnn_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_rnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-model Embedding\n",
    "Below are variations of in-model embedding with different NN layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding dimension\n",
    "embedding_dim = 50 \n",
    "\n",
    "# define input layer\n",
    "input_layer = tf.keras.layers.Input(shape=(max_text_length,))\n",
    "\n",
    "# embedding layer\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_text_length)(input_layer)\n",
    "\n",
    "# to reshape LSTM & RNN to 1 timestep\n",
    "reshaped = tf.keras.layers.Reshape((1, -1))(embedding_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "lstm_layer = tf.keras.layers.LSTM(units=64)(reshaped)\n",
    "\n",
    "# output layer\n",
    "output1 = tf.keras.layers.Dense(len(label_dict), activation='softmax')(lstm_layer)\n",
    "\n",
    "# define model\n",
    "emb_model1 = tf.keras.Model(inputs=input_layer, outputs=output1)\n",
    "\n",
    "# compile , train & evaluate the model\n",
    "emb_model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "emb_model1.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "emb_loss1, emb_accuracy1 = emb_model1.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", emb_accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model1.summary()\n",
    "emb_model1.save(\"models_p1/emb/emb_lstm.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RNN layer\n",
    "rnn_layer = tf.keras.layers.SimpleRNN(units=64)(reshaped)\n",
    "\n",
    "# output layer\n",
    "output2 = tf.keras.layers.Dense(len(label_dict), activation='softmax')(rnn_layer)\n",
    "\n",
    "# define model\n",
    "emb_model2 = tf.keras.Model(inputs=input_layer, outputs=output2)\n",
    "\n",
    "# compile , train & evaluate the model\n",
    "emb_model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "emb_model2.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "emb_loss2, emb_accuracy2 = emb_model2.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", emb_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense + Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer\n",
    "flattened = tf.keras.layers.Flatten()(embedding_layer) # to convert multi-D data into 1D tensor\n",
    "dense1 = tf.keras.layers.Dense(units=64)(flattened)\n",
    "\n",
    "# output layer\n",
    "output3 = tf.keras.layers.Dense(len(label_dict), activation='softmax')(dense1)\n",
    "\n",
    "# define MODEL\n",
    "emb_model3 = tf.keras.Model(inputs=input_layer, outputs=output3)\n",
    "\n",
    "# compile , train & evaluate the model\n",
    "emb_model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "emb_model3.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "emb_loss3, emb_accuracy3 = emb_model3.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", emb_accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-trained embeddings\n",
    "Using the gensim library, I will pretrain a Word2Vec model on the lyric data and train same set of models to compare with in-model embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# select lyrics & split into sentences\n",
    "lyrics_list = music_df[\"Prsd_Lyrics\"].tolist()\n",
    "sentences = [lyric.split() for lyric in lyrics_list]\n",
    "\n",
    "# train the Word2Vec model\n",
    "w2v_pmodel = Word2Vec(sentences=sentences, window=5, min_count=1, workers=4)\n",
    "\n",
    "# save model\n",
    "w2v_pmodel.save(\"models_p1/word2vec_model\")\n",
    "\n",
    "# define vocab_size & embedding dimension\n",
    "embedding_dim = 100 \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# load pre-trained Word2vec embedding weights\n",
    "word2vec_model = gensim.models.Word2Vec.load(\"models_p1/word2vec_model\")\n",
    "w2v_embedding_matrix = np.zeros((vocab_size, embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input layer\n",
    "w2v_input_layer = tf.keras.layers.Input(shape=(max_text_length,))\n",
    "\n",
    "# define embedding layer\n",
    "w2v_embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_text_length, weights=[w2v_embedding_matrix])(w2v_input_layer)\n",
    "\n",
    "# to reshape LSTM & RNN to 1 timestep\n",
    "w2v_reshaped = tf.keras.layers.Reshape((1, -1))(w2v_embedding_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM + Pre-trained Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "lstm_layer2 = tf.keras.layers.LSTM(units=64)(w2v_reshaped)\n",
    "\n",
    "# output layer\n",
    "w2v_output1 = tf.keras.layers.Dense(len(label_dict), activation='softmax')(lstm_layer2)\n",
    "\n",
    "# define model\n",
    "w2v_model1 = tf.keras.Model(inputs=w2v_input_layer, outputs=w2v_output1)\n",
    "\n",
    "# compile , train & evaluate the model\n",
    "w2v_model1.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "w2v_model1.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "w2v_loss1, w2v_accuracy1 = w2v_model1.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", w2v_accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model1.summary()\n",
    "w2v_model1.save(\"models_p1/emb/w2v_lstm.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN + Pre-trained Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple RNN layer\n",
    "rnn_layer2 = tf.keras.layers.SimpleRNN(units=64)(w2v_reshaped)\n",
    "\n",
    "# output layer\n",
    "w2v_output2 = tf.keras.layers.Dense(len(label_dict), activation='softmax')(rnn_layer2)\n",
    "\n",
    "# define model\n",
    "w2v_model2 = tf.keras.Model(inputs=w2v_input_layer, outputs=w2v_output2)\n",
    "\n",
    "# compile , train & evaluate the model\n",
    "w2v_model2.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "w2v_model2.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "w2v_loss2, w2v_accuracy2 = w2v_model2.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", w2v_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dense + Pre-trained Word2Vec embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense layer\n",
    "w2v_flattened = tf.keras.layers.Flatten()(w2v_embedding_layer) # to convert multi-D data into 1D tensor\n",
    "dense2 = tf.keras.layers.Dense(units=64)(w2v_flattened)\n",
    "\n",
    "# output layer\n",
    "w2v_output3 = tf.keras.layers.Dense(len(label_dict), activation='softmax')(dense2)\n",
    "\n",
    "# pre_trained embedding model\n",
    "w2v_model3 = tf.keras.Model(inputs=w2v_input_layer, outputs=w2v_output3)\n",
    "\n",
    "# compile , train & evaluate the model\n",
    "w2v_model3.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy']) \n",
    "w2v_model3.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "w2v_loss3, w2v_accuracy3 = w2v_model3.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", w2v_accuracy3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional layers\n",
    "Experimenting with convolutional layers-only models and mixing in LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN model\n",
    "cnn_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=7, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(units=len(label_dict), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# compile model\n",
    "cnn_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train & evalutate model\n",
    "cnn_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "loss, accuracy = cnn_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define CNN + LSTM model\n",
    "cnnlstm_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Reshape((-1, 32)), # reshape for LSTM input\n",
    "    tf.keras.layers.LSTM(units=16, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(units=32),\n",
    "    tf.keras.layers.Dense(units=len(label_dict), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# compile model\n",
    "cnnlstm_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train & evalutate model\n",
    "cnnlstm_model.fit(X_train, y_train, epochs=10, validation_split=0.2, callbacks=[early_stopping])\n",
    "loss1, accuracy1 = cnnlstm_model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", accuracy1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
